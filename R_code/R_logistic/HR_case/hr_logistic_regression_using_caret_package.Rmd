---
title: "Logistic Regression using Caret Package"
author: "Kumar Rahul"
date: "9 September 2016"
output:
  html_document:
    df_print: paged
---

## In this exercise, we will use the HR dataset and understand the following using caret package:

> 1. Building the logistic regression model
2. What is marked as the positive class by the model when using caret package
3. Feature selection using caret package
3. Writing the model equation and interpreting the model summary
4. Creating the Confusion Matrix and ROC plot on train data
5. Using mis-classification cost as a criteria to select the best cut-off
6. Using Younden Index as the criteria to select the best cut-off
7. Creating the Confusion Matrix and ROC plot on test data
8. Compare and discuss the result of logistic regression using caret vis-a-via stats package
9. Changing the base or reference category and evaluate the impact on the model (This is self work/assignment)
10. Change the cut-off value for train data in caret package (This is self work/assignment)

There are bugs/missing code in the entire exercise. The participants are expected to work upon them.
***
***

## Here are some useful links:

> 1. **[Read](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm)** about interaction variable coding
2. Refer **[link](http://www.statmethods.net/input/valuelabels.html)** to know about adding lables to factors
3. Refer **[link](http://stackoverflow.com/questions/2342472/recode-relevel-data-frame-factors-with-different-levels)** to relvel factor variables
4. **[Read](http://stats.stackexchange.com/questions/88485/variable-is-significant-through-stepwise-regression-but-not-in-final-models-sum)** about the issues in stepwise regression
5. **[Read](http://topepo.github.io/caret/training.html)** about the modelling activity via caret package
6. The **[complete](http://topepo.github.io/caret/available-models.html)** list of tuning parameter for different models in caret package

***

# Code starts here
We are going to use below mentioned libraries for demonstrating logistic regression:

```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(caret)    #for data partition. Model building
#library(Deducer)  #for ROC plot
library(ROCR)     #for ROC plot (other way)
```

## Data Import and Manipulation

### 1. Importing a data set

_Give the correct path to the data_

```{r readData, echo=TRUE,tidy=TRUE}
raw_df <- read.csv("/Users/Rahul/Documents/Datasets/IMB533_HR_Data_No_Missing_Value.csv", header = TRUE,sep = ",",na.strings = c(""," ", "NA"))

raw_df = raw_df[,-c(1,2)]
```

Note that `echo = FALSE` parameter prevents printing the R code that generated the
plot.

### 2. Structure and Summary of the dataset

```{r summarizeData, echo=TRUE,tidy=TRUE}
str(raw_df)
summary(raw_df)
```
## Data Cleaning

### Zero and Near Zero Variance Features

> 1. The data generating mechanism can create predictors that only have a single unique value (i.e. a “zero-variance predictor”).
2. Predictors might have only a handful of unique values that occur with very low frequencies. These may create problem when data is split using CV/bootstrap.

To identify these types of predictors, the following two metrics can be calculated:

> 1. the frequency of the most prevalent value over the second most frequent value (called the “frequency ratio’’), which would be near one for well-behaved predictors and very large for highly-unbalanced data and
2. the "percent of unique values" is the number of unique values divided by the total number of samples (times 100) that approaches zero as the granularity of the data increases

If the frequency ratio is greater than a pre-specified threshold and the unique value percentage is less than a threshold, we might consider a predictor to be near zero-variance.

More at: http://topepo.github.io/caret/pre-processing.html#zero--and-near-zero-variance-predictors

```{r}
nzv <- nearZeroVar(raw_df, saveMetrics= TRUE)
nzv
```

Create a new data frame and store the raw data copy. This is being done to have a copy of the raw data intact for further manipulation if needed.

```{r createDataCopy, echo=TRUE,tidy=TRUE}
dim(raw_df)
nzv <- nearZeroVar(raw_df, saveMetrics= FALSE)
filter_df = raw_df[,-nzv]

filter_df = na.omit(filter_df)
dim(filter_df)
```
### Dummy Variable coding

```{r}
dummies = dummyVars(~ ., data = filter_df)
filter_with_dummy_df = predict(dummies, newdata = filter_df)
#head(filter_with_dummy_df)
```

### Linear Dependencies

The function `findLinearCombos` uses the QR decomposition of a matrix to enumerate sets of linear combinations (if they exist).


```{r}
feature_map <- unlist(lapply(filter_df, is.numeric)) 

linear_depend_info = findLinearCombos((filter_df[,feature_map]))
linear_depend_info
```

### 3. Create train and test dataset

#### Reserve 80% for **_training_** and 20% of **_test_**

```{r createDataPartition, echo=TRUE,tidy=TRUE}
set.seed(2341)
trainIndex <- createDataPartition(filter_df$Status, p = 0.80, list = FALSE)
train_df <- filter_df[trainIndex,]
test_df <- filter_df[-trainIndex,]
```

We can pull the specific attribute needed to build the model is another data frame. This agian is more of a hygine practice to not touch the **train** and **test** data set directly.

```{r}
predictors = names(train_df)[!names(train_df) %in% c('Candidate.relocate.actual','Status')]
response_variable = c('Status')
predictors
```

***

## Model Building: Using the **caret()** package
There are a number of models which can be built using caret package. To get the names of all the models possible.

```{r caretModelInfo, echo=TRUE}
names(getModelInfo())
```

To get the info on specific model:

```{r caretModelType, echo=TRUE}
getModelInfo()$glmStepAIC$type
```

To find the paramters of a model which can be tuned in caret

```{r}
modelLookup(model='glmStepAIC')
```

The below chunk of code is standarized way of building model using caret package. Setting in the control parameters for the model.

```{r caretControl, echo=TRUE}
set.seed(1234)
objControl <- trainControl(method = "none",
                           summaryFunction = twoClassSummary,
                           #summaryFunction = twoClassSummary, defaultSummary
                           classProbs = TRUE,
                           savePredictions = TRUE)
```

The search grid is basically a model fine tuning option. The paramter inside the **expan.grid()** function varies according to model. The **[complete](http://topepo.github.io/caret/available-models.html)** list of tuning paramter for different models.

```{r caretTune, echo=TRUE}
#This parameter is for glmnet. Need not be executed if method  is glmStepAIC
#searchGrid <-  expand.grid(alpha = c(1:10)*0.1,
#                           lambda = c(1:5)/10)
```

The model building starts here.
> 1. **metric= "ROC"** uses ROC curve to select the best model.Accuracy, Kappa are other options. To use this change twoClassSummary to defaultSummary in **ObjControl**
2. **verbose = FALSE**: does not show the processing output on console

The factor names at times may not be consistent. R may expect **"Not.Joined"** but the actual level may be **"Not Joined"** This is corrected by using **make.names()** function to give syntactically valid names.

```{r caretModel, echo=TRUE, message=FALSE, warning=FALSE}
#train_df$StatusFactor <- as.factor(ifelse(train_df$Status == "Joined", 1,0))
set.seed(766)
levels(train_df$Status)  = make.names(levels(factor(train_df$Status)))
caret_model <- train(train_df[,predictors],
                      train_df[,response_variable],
                      method = 'glmStepAIC', 
                      family=binomial(), #'glm', glmnet
                      trControl = objControl,
                      metric = "ROC",
                      verbose = FALSE)
```

## Model Evaluation

### 1. One useful plot from caret package is the variable importance plot

In case you get an error "Invalid Graphic state", uncomment the line below

```{r caretVarImp, echo=TRUE}
caret_model
summary(caret_model$finalModel)

#dev.off()
#plot(varImp(caret_model, scale = TRUE))
```

### 2. The prediction and confusion Matrix on train data.

The syntax for prediction in caret is almost similar expect the the **type** attribute expects input as **'raw'** or **'prob'**. In case of prob, the predicted value holds the probability of both positive and negative class.

```{r caretPrediction, echo=TRUE}
levels(train_df$Status) <- make.names(levels(factor(train_df$Status)))
caretPredictedClass <- predict(object = caret_model, train_df[,predictors], type = 'raw')
confusionMatrix(caretPredictedClass,train_df$Status)
```

### 3. The optimal cut-off

Creating empty vectors to store the results.

```{r variableDeclaration, echo=TRUE}
msclaf_cost <- c()
youden_index <- c()
cutoff <- c()
P11 <- c() #correct classification of positive as positive
P00 <- c() #correct classification of negative as negative
P10 <- c() #misclassification of positive class to negative class
P01 <- c() #misclassification of negative class to positive class
```

#### Select the optimal cut-off value, if:

> 1. cost of misclassifying Not Joined as Joined is twice as costly as cost of
micalssifying Joined as Not Joined
2. both sensitivity and specificity are equally important

The best cut-off is the one which minimizes the misclassification cost (in case of **_option 1_**) or which maximizes the Youden's Index (in case of **_Option 2_**).

_fix the bug here_: clue is in the above **two options**

```{r modelOptimalCutOff, echo=TRUE, tidy=TRUE}
train_predicted_prob = predict(object = caret_model, train_df[,predictors], type = 'prob')
#variable with all the values as joined
n <- length(train_df$Status)

costs = matrix(c(0,2,1, 0), ncol = 2)
colnames(costs) = rownames(costs) = c("Joined", "Non Joined")
as.table(costs)
```

The misclassification cost table is:

```{r costCal, echo=TRUE, tidy=TRUE}
# defining log odds in favor of Joined
for (i in seq(0.05, 1, .05)) {
  predicted_y = rep("Not Joined", n)
  predicted_y[train_predicted_prob[1] > i] = "Joined"
  tbl <- table(train_df$Status, predicted_y)
  if ( i <= 1) {
    #Classifying Not Joined as Joined
    P10[20*i] <- tbl[2]/(tbl[2] + tbl[4])

    P11[20*i] <- tbl[4]/(tbl[2] + tbl[4])

    #Classifying Joined as Not Joined
    P01[20*i] <- tbl[3]/(tbl[1] + tbl[3])

    P00[20*i] <- tbl[1]/(tbl[1] + tbl[3])

    cutoff[20*i] <- i
    msclaf_cost[20*i] <- P10[20*i]*costs[2] + P01[20*i]*costs[3]
    youden_index[20*i] <- P11[20*i] + P00[20*i] - 1
  }
}
df.cost.table <- cbind(cutoff,P10,P01,msclaf_cost, P11, P00, youden_index)
```

The table summarizing the optimal cut-off value:

_write the cost.table into a csv file_

```{r modelOptimalCutoffTable, echo=TRUE}
df.cost.table
#write.csv(df.cost.table, "Optimal_Cutoff_caret.csv")
```

### 4. Confusion Matrix on the test data

The **predict** function is used to get the predicted probability on the new dataset. The probability value along with the optimal cut-off can be used to build confusion matrix

```{r modelValidation, echo=TRUE, tidy=TRUE}
test_predicted_prob = predict(caret_model, test_df, type = "prob")

#variable with all the values as joined
n <- length(test_df$Status)
predicted_y = rep("Not Joined", n)

# defining log odds in favor of not joining
predicted_y[test_predicted_prob[1] > 0.80] = "Joined"

#add the model_precition in the data
test_df$predicted_y <- predicted_y

###Create the confusionmatrix###
addmargins(table(test_df$Status, test_df$predicted_y))
mean(test_df$predicted_y == test_df$Status)
```

### 5. ROC Plot on the test data

ROCR package can be used to evaluate the model performace on the test data. The same package can also be used to get the model performace on the test data.

```{r validationROC, echo=TRUE, tidy=TRUE}
lgPredObj <- prediction(test_predicted_prob[2],test_df$Status)
lgPerfObj <- performance(lgPredObj, "tpr","fpr")
plot(lgPerfObj,main = "ROC Curve",col = 2,lwd = 2)
abline(a = 0,b = 1,lwd = 2,lty = 3,col = "black")
performance(lgPredObj, "auc")
```

#### End of Document

***
***
