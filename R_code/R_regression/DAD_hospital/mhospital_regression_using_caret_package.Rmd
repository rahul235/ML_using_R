---
title: "Regression Concepts Using R"
author: "Kumar Rahul"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## In this exercise, we will use the patient data and understand the following:

> 1. Importing the datset from a csv file
2. Understanding the strucutre and summary of the data
3. Typecasting a variable to a proper data type
4. Creating derived variables and interaction variables
5. Analyzing the corelation amongst variables
6. Releveling the factor variable and understand its impact
7. Building the regression model using caret package
8. Writing the model equation and interpreting the model summary
9. Analayzing the statistics to acertain the validity of the model

There are bugs/missing code in the entire exercise. The participants are expected to work upon them.
***
***

## Here are some useful links:

> 1. Refer [link](http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm) to know more about different ways of dummy variable coding
2. [Read](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm) about interaction variable coding
3. Refer [link](http://www.statmethods.net/input/valuelabels.html) to know about adding lables to factors
4. Refer [link](http://stackoverflow.com/questions/2342472/recode-relevel-data-frame-factors-with-different-levels) to relevel factor variables
5. [Read](http://stats.stackexchange.com/questions/88485/variable-is-significant-through-stepwise-regression-but-not-in-final-models-sum) about the issues in stepwise regression
6. The issues arising out of multi-colinearity is discussed  [here](http://blog.minitab.com/blog/understanding-statistics/handling-multicollinearity-in-regression-analysis) or  [here](https://onlinecourses.science.psu.edu/stat501/node/343)
7. The residual diagonstic can be interpreted from [here](http://data.library.virginia.edu/diagnostic-plots/)
8. [Read](https://onlinecourses.science.psu.edu/stat501/node/337) to understand the distinction between **outliers** and **influential cases**
9. [Change](http://stackoverflow.com/questions/16819956/invalid-factor-level-na-generated) NAs to a new label
10. [Sampling](http://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package/) of data can be tricky and change the outcome of the model.
11. Issues with rJava installation may get resolved by following [link](https://www.r-statistics.com/2012/08/how-to-load-the-rjava-package-after-the-error-java_home-cannot-be-determined-from-the-registry/) or by [link](http://stackoverflow.com/questions/27661325/unable-to-load-rjava-on-r)

***

# Code starts here

We are going to use below mentioned libraries for demonstrating regression:

We are going to use **stats** and **caret** packages for demonstrating linear regression. 

```{r libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(stats)    #for regression
library(caret)    #for data partition
library(car)      #for VIF
library(sandwich) #for variance, covariance matrix
#library(jtools) #for visualizaling interaction variable. Not being used here.
```

## Data Import and Manipulation

### 1. Importing a data set

make.names() makes syntactically valid names out of character vectors. A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number. unique=TRUE allows to avoid possible dublicates in new column names.

_Give the correct path to the data_
```{r readData, echo=TRUE,tidy=TRUE}
raw_df <- read.csv("/Users/Rahul/Documents/Datasets/Mission Hospital-Case Data.csv", header = TRUE,sep = ",",na.strings = c(""," ", "NA"), stringsAsFactors = TRUE)
names(raw_df) = make.names(names(raw_df) , unique=TRUE)
raw_df <- raw_df[,c(-58:-62)]

```

Note that `echo = FALSE` parameter prevents printing the R code that generated the
plot.

### 2a. Structure and Summary of the dataset
There are 175 NA values in Past Medical History Code. However, rather than treating these as missing values, it represents that there is no past medical history for these patients. These NA may be marked as "None". But while doing so, the code will give an error as we are trying to add a new level to factor variable (**raw_df$Past.MEDICAL.HISTORY.CODE**). In order to add a new level, first we will need to typecast this variable as a character variable, add a new level and then re-typecast them as Factor variable.

```{r summarizeData, echo=TRUE,tidy=TRUE}
#str(raw_df)
#summary(raw_df)
```

```{r mergelables, echo=TRUE,tidy=TRUE}
raw_df$PAST.MEDICAL.HISTORY.CODE[raw_df$PAST.MEDICAL.HISTORY.CODE == "Hypertension1"] <- "hypertension1"

raw_df$PAST.MEDICAL.HISTORY.CODE <- as.character(raw_df$PAST.MEDICAL.HISTORY.CODE)

raw_df$PAST.MEDICAL.HISTORY.CODE[is.na(raw_df$PAST.MEDICAL.HISTORY.CODE)] <- "None"

raw_df$PAST.MEDICAL.HISTORY.CODE <- as.factor(raw_df$PAST.MEDICAL.HISTORY.CODE)
```

Create a new data frame and store the raw data copy. This is being done to have a copy of the raw data intact for further manipulation if needed.

```{r createDataCopy, echo=TRUE,tidy=TRUE}
new_df <- raw_df[,c(-1,-4,-5,-7,-9:-21,-23,-25,-31:-36,-41,-42,-44,-46,-48,-56)]
new_df <- na.omit(new_df) # listwise deletion of missing
```

### 3a. Correlation among Variables

From the numeric attribute in the data, it will of interest to analyze the variables which are corelated to each other. High corelation amongst variable may result in the issue of **multi-colinearity** in the model.

```{r corMatrix, echo=TRUE,tidy=TRUE}
correlation_matrix <- cor(new_df[,c(1,7:10,12:14,18:24,26)])
correlation_matrix
# find attributes that are highly corrected (ideally >0.7)
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.7, names = TRUE)
print(highly_correlated)
```

### 3b. Derived variables
Deriving BMI to drop of Weight and Height as variables. Both of them where highly corelated to age. Droping Cretanine as a variable as it is highly corleated to age.

```{r}
new_df$BMI <- new_df$BODY.WEIGHT/((new_df$BODY.HEIGHT/10) ^ 2)
filter_df <- new_df[,c(-5:-6)]
```

### 3c. Relevel

By default, the base category/reference category selected is ordered alphabetically. In this code chunk we are just changing the base category for PAST.MEDICAL.HISTORY.CODE variable.

The base category can be releveled using the function **relevel()**.

```{r relevelCategory, echo=TRUE,tidy=TRUE}
filter_df$PAST.MEDICAL.HISTORY.CODE <- relevel(filter_df$PAST.MEDICAL.HISTORY.CODE, ref = "None")
```

### 4. Create train and test dataset

#### Reserve 80% for **_training_** and 20% of **_test_**

_Correct the error in the below code chunk_

```{r createDataPartition, echo=TRUE,tidy=TRUE}
set.seed(2341)
index <- createDataPartition(filter_df$TOTAL.COST.TO.HOSPITAL, p = 0.8, list = FALSE)
train_df <- filter_df[index,]
test_df <- filter_df[-index,]
```

Transformation of variables may be needed to validate the model assumptions.

```{r}
train_df$Log.Cost.Treatment <- log(train_df$TOTAL.COST.TO.HOSPITAL)
test_df$Log.Cost.Treatment <- log(test_df$TOTAL.COST.TO.HOSPITAL)
```

We can pull the specific attribute needed to build the model in another data frame. This again is more of a hygine practice to not touch the **train** and **test** data set directly.

```{r variableUsedinTraining, echo=TRUE,tidy=TRUE}
reg_train_df <- as.data.frame(train_df[,c("AGE",
                                             "BMI",
                                             "COST.OF.IMPLANT",
                                             "IMPLANT.USED",
                                             "GENDER",
                                             "MARITAL.STATUS",
                                             "PAST.MEDICAL.HISTORY.CODE",
                                             "MODE.OF.ARRIVAL",
                                             "TYPE.OF.ADMSN",
                                             "TOTAL.COST.TO.HOSPITAL"
)])
```


```{r variableUsedinTesting, echo=TRUE, tidy=TRUE}
reg_test_df <- as.data.frame(test_df[,c("AGE",
                                             "BMI",
                                             "COST.OF.IMPLANT",
                                             "IMPLANT.USED",
                                             "GENDER",
                                             "MARITAL.STATUS",
                                             "PAST.MEDICAL.HISTORY.CODE",
                                             "MODE.OF.ARRIVAL",
                                             "TYPE.OF.ADMSN",
                                             "TOTAL.COST.TO.HOSPITAL"
)])
```

***

## Model Building: Using the **caret()** package
There are a number of models which can be built using caret package. To get the names of all the models possible.

```{r caretModelInfo, echo=TRUE}
names(getModelInfo())
```

To get the info on specific model:

```{r caretModelType, echo=TRUE}
getModelInfo()$xyf$type
```

The below chunk of code is standarized way of building model using caret package. Setting in the control parameters for the model.
Cross validation sample with k folds will split the data into equal sized sample. The model will be repeatedly built on k-1 folds and tested on left out fold. The error reported in the model is an average error across all the models.

```{r caretControl, echo=TRUE}
objControl <- trainControl(method = "none",
                           summaryFunction = defaultSummary,
                           #summaryFunction = twoClassSummary, defaultSummary
                           classProbs = FALSE,
                           savePredictions = TRUE)
```

The search grid is basically a model fine tuning option. The paramter inside the **expan.grid()** function varies according to model. The **[complete](http://topepo.github.io/caret/modelList.html)** list of tuning paramter for different models.

```{r caretTune, echo=TRUE}
#This parameter is for glmnet. Need not be executed if method  is lmStepAIC
searchGrid <-  expand.grid(alpha = c(1:10)*0.1,
                           lambda = c(1:5)/10)
```

The factor names at times may not be consistent. This is corrected by using **make.names()** function to give syntactically valid names.

```{r caretModel, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(766)

reg_caret_model <- train(reg_train_df[,1:9],
                      reg_train_df[,10],
                      method = 'lmStepAIC', #lmStepAIC, 
                      trControl = objControl,
                      #metric = "Rsquared",
                      #tuneGrid = searchGrid
                      )
```

### AIC Criteria for model selection

In order to overcome the issue of running the regression model in iterations, caret package has a method lmStepAIC which implements the strategy of variable selection and elimation by looking at Akaike information criterion.
As the name suggests, lmStepAIC is a step function which uses Akaike information criterion (AIC) as a criteria for model selection. AIC estimates the quality of each model, relative to each of the other models for a given set of variables. Thus, AIC provides a means for model selection.

**AIC does not provide a test of a model in terms of null hypothesis and thus it does not tells anything about the absolute quality of a model but only the quality relative to other models. AIC will not give any warning if all the candidate models fit poorly.**

Formula to compute AIC is:

$AIC = n ∗ ln RSS + k ∗ P$

where: 

> * n is the number of observations in the data: nrow(reg_train_df)
* RSS is the regression sum of squares: $sum((reg_caret_model$finalModel$residuals)^2)$
* k is the penalizing factor. Only k = 2 gives the genuine AIC: k = log(n) is sometimes referred to as BIC or SBC. 
* P is the number of parameters in the model : (length(reg_caret_model$finalModel$coefficients) )

## Model Evaluation

### 1. One useful plot from caret package is the variable importance plot

```{r caretVarImp, echo=TRUE}
plot(varImp(reg_caret_model, scale = TRUE))
```

Checking the if the model satisfies the assumpations of Linear Regression Model. Note that this evaluation is on training data.

The model summary gives the equation of the model as well as helps test the assumption that beta coeffiecents are not statically zero.

```{r modelStats,tidy=TRUE}
summary(reg_caret_model)
```

### 2. The residual analysis

The error term diagnostic is critical to understanding the behaviour of linear regression models. The two critical assumptions of linear regression are:

>1. Error term should be normally distributed
2. Error term should have constant variance (**homoscedasticity**)

The **plot()** function when used on the regression object model gives us four different plots. The two important one to analyze there are:

1. Normal Q-Q
2. Scale-Location

##### Normal Q_Q plot
This plot shows if the error terms are normally distributed. In case, of normal distribution, the dots should appear close to the straight line with not much of a deviation.

##### Scale-Location
Also known as spread location plot, it shows if the residuals are equally spread along the range of predictors. It is desirable to see a horizontal straight line with with randomly spread points.

**The other two plots are:**

##### Residual vs. Fitted
There could be a non linear relationship between predictor variable (Xs) and the outcome variable (Y). This non linear relationship can show up in this plot which may suggest that the model is mis-specified. It is desirable to see a horizontal straight line with with randomly spread points.

##### Residual vs. Leverage
The regression line can be influenced by outliers (extreme values in Y) or by data points with high leverage (extreme values in X). Not all the extreme values are influential cases in regression analysis.

Even if data has extreme values, it may not be influential to determine the regression line. On the flip side, some cases could be very influential even if they do not seem to be an outlier. Influential cases are identified by cook's distance. In the plot, look for for outlying values at the upper right corner or at the lower right corner (cases outside of a dashed line i.e. Cook’s distance).

```{r variableDeclaration, echo=TRUE}
plot(reg_caret_model$finalModel)

#hist(residuals(RegModelStepwise), main = "Residuals", col = 'blue')
```

##### Visual inspection to check for heteroscedasticity in error terms

You may ignore the below code chuck. This is an elaboration of the scale-location plot obtained before.

```{r modelOptimalCutOff, echo=FALSE,tidy=TRUE}
plot(predict(reg_caret_model$finalModel), residuals(reg_caret_model$finalModel), main = "Scale-Location")
#yhat <- RegModelStepwise$fitted.values
#plot(yhat, res) #same plot as above
```

##### Multi-colinearity

Variance Inflation Factor (VIF) is a measure of how much the variance of the estimated regrression coeffiecients are inflated as compared to when the predicator variable are not linearly related.

> VIF = 1 : Not Correlated
> 1<VIF<5 : Moderately Correlated
> 5<VIF<=10: Highly Correlated

_The square root of the VIF tells you how much larger the standard error is, compared with what it would be if that variable were uncorrelated with the other predictor variables in the model._

Say, if the square root of the VIF is 2.5; this means that the standard error for the coefficient of that predictor variable is 2.5 times as large as it would be if the predictor variable were uncorrelated with the other predictor variables

Generally the issue of multi-colinearity wil not arise, if the corelation amongst variable has been analyzed before model building and the one amongst the corelated variable has been dropped from the data.

> High values of VIF can be ignored when used interaction effect. More at this [link](https://statisticalhorizons.com/multicollinearity)

```{r}
vif(reg_caret_model$finalModel)
```

### 3. Model Validation on the Test Data

The **predict** function is used to get the predicted response on the new dataset.
You may get an error message if the test data has got any new levels which was not there in the training set. This generally happens when the data has categorical variable with multiple levels.

```{r modelValidation, echo=TRUE,tidy=TRUE}
reg_test_df$IMPLANT.USED = make.names(factor(reg_test_df$IMPLANT.USED))
reg_test_df_predict = predict(reg_caret_model$finalModel, reg_test_df,
                            interval = "confidence",
                            level = 0.95,
                            type = "response")
data.frame(reg_test_df_predict, reg_test_df$TOTAL.COST.TO.HOSPITAL)
```

## Interaction variable - Optional

Suppose we want to make a model with TOTAL.COST.TO.HOSPITAL, AGE and PAST.MEDICAL.HISTORY.CODE.

A simple regression model will be 

1. $TOTAL.COST.TO.HOSPITAL = \beta0 + \beta_1*AGE + \beta_2*PAST.MEDICAL.HISTORY.CODE$

However, the above form does not test the hypothesis about the relationship between the Age of the pateint, on the cost of treatment, for different past medical conditions:

> Diabetic condition in older patients may lead to more cost of treatment for heart surgery whereas the same age group patient with no diabetic condition may incur lesser cost of treatment.

The functional form for such tests will be

2. $TOTAL.COST.TO.HOSPITAL = \beta0 + \beta_1*AGE + \beta_2*PAST.MEDICAL.HISTORY.CODE + \beta_3*AGE*PAST.MEDICAL.HISTORY.CODE$

The presence of a significant interaction indicates that the effect of one predictor variable on the response variable is different at different values of the other predictor variable. Adding an interaction term to a model drastically changes the interpretation of all the coefficients. If there were no interaction term, $\beta_1$ would be interpreted as the unique effect of Age on cost of treatment. But the interaction means that the effect of Age on cost of treatment is different for different past medical condition.  So the unique effect of Age on cost of treatment is not just limited to $\beta_1$ but also depends on the values of $\beta_3$ and past medical condition. 

* The unique effect of Age is represented by everything that is multiplied by Age in the model: $\beta_1 +\beta_3*PAST.MEDICAL.HISTORY.CODE$. 
* $\beta_1$ is now interpreted as the unique effect of Age on cost of treatment only when PAST.MEDICAL.HISTORY.CODE = 0 (base category).

The above model can be built by passing the features in formula form. Different forms in the below code chunk is for testing interactions between `AGE` and `PAST.MEDICAL.HISTORY.CODE`

```{r formulaModel, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(766)

form_1 = as.formula('TOTAL.COST.TO.HOSPITAL~ .')
form_2 = as.formula('TOTAL.COST.TO.HOSPITAL~ . + AGE:PAST.MEDICAL.HISTORY.CODE')
form_3 = as.formula('TOTAL.COST.TO.HOSPITAL~ AGE*PAST.MEDICAL.HISTORY.CODE')

form_4 = as.formula('TOTAL.COST.TO.HOSPITAL~  AGE:GENDER')

reg_caret_model_2 <- caret:::train(TOTAL.COST.TO.HOSPITAL~  AGE*GENDER,data= reg_train_df,
                      method = 'lm', #lmStepAIC, BstLm, 
                      trControl = objControl,
                      metric = "Rsquared",
                      #tuneGrid = searchGrid
                      )
```

### Visualize interaction

An interaction plot is a line graph that reveals the presence or absence of interactions among independent variables. To create an interaction plot, do the following:

> * Show the dependent variable on the vertical axis (i.e., the Y axis); and an independent variable, on the horizontal axis (i.e., the X axis).
* Plot mean scores on the dependent variable separately for each level of a potential interacting variable.
* Connect the mean scores, producing separate lines for each level of the interacting variable

To understand potential interaction effects, compare the lines from the interaction plot:

> * If the lines are parallel, there is no interaction.
* If the lines are not parallel, there is an interaction.
* More about interaction variables  [here](https://stattrek.com/multiple-regression/interaction.aspx) and in this [link](https://cran.r-project.org/web/packages/interactions/vignettes/interactions.html).

interact_plot() is used to visulaize the interaction

> * `interact_plot(reg_caret_model_2, pred = AGE, modx = PAST.MEDICAL.HISTORY.CODE)`

```{r}
library(jtools)
library(interactions)

#interact_plot(reg_caret_model_2$finalModel, pred = AGE, modx =GENDER,data = #reg_train_df)

```

***
***
